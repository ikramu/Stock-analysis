{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('anaconda3': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "61ed3d88697b33e1090d1b524ac8eb932b91b725375fa19f7ac5b6d2cce1f020"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import argparse\n",
    "import yaml\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, save_model, load_model\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "from keras import optimizers\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from trainDataReader import TrainDataReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer(object):\n",
    "    \"\"\" super class for reader classes for training and inference \n",
    "    \"\"\" \n",
    "    def __init__(self, dbHost, dbUser, dbPasswd, dbName,dbTable, stocks, snapshotDir):\n",
    "        super().__init__()\n",
    "        self._dbHost = dbHost\n",
    "        self._dbUser = dbUser\n",
    "        self._dbPasswd = dbPasswd\n",
    "        self._dbName = dbName\n",
    "        self._dbTable = dbTable\n",
    "        self._stocks = stocks\n",
    "        self._snapshotDir = snapshotDir\n",
    "        self._tdReader = TrainDataReader(dbHost = dbHost, dbUser = dbUser, dbPasswd = dbPasswd, \n",
    "        dbName = dbName, dbTable = dbTable, stocks = stocks)\n",
    "\n",
    "    def readTrainingData(self, end_date):\n",
    "        trData = self._tdReader.readStockData(end_date)\n",
    "        return trData\n",
    "    \n",
    "    def normalizeDataAndSave(self, train, valid):\n",
    "        #train_cols = train.columns[1:]\n",
    "        train_cols = train.columns\n",
    "        \n",
    "        # scale the feature MinMax, build array\n",
    "        x = train.loc[:,train_cols].values\n",
    "        self._min_max_scaler = MinMaxScaler()\n",
    "        x_train = self._min_max_scaler.fit_transform(x)\n",
    "        x_valid = self._min_max_scaler.transform(valid.loc[:,train_cols])\n",
    "\n",
    "        train.to_pickle(os.path.join(self._snapshotDir,\"dfTrain.pkl\"))\n",
    "        valid.to_pickle(os.path.join(self._snapshotDir,\"dfValid.pkl\"))\n",
    "        np.save(os.path.join(self._snapshotDir,\"x_train\"), x_train)\n",
    "        np.save(os.path.join(self._snapshotDir,\"x_valid\"), x_valid)\n",
    "        joblib.dump(self._min_max_scaler, os.path.join(self._snapshotDir,\"minMaxScaler.pkl\"))\n",
    "        \n",
    "        return x_train, x_valid, self._min_max_scaler\n",
    "\n",
    "    def buildTimeseries(self, mat, pred_len, pred_step, pred_col_id, time_steps, batchSize):\n",
    "        # We skip the pred_step so it needs to be catered in for-loop\n",
    "        # pred_col_id is the index of column that would act as output column\n",
    "        # total number of time-series samples would be len(mat) - TIME_STEPS\n",
    "        dim_0 = mat.shape[0] - time_steps - pred_step - pred_len + 1\n",
    "        dim_1 = mat.shape[1]\n",
    "        x = np.zeros((dim_0, time_steps, dim_1))\n",
    "        y = np.zeros((dim_0,pred_len))\n",
    "        \n",
    "        for i in tqdm_notebook(range(x.shape[0])):\n",
    "            x[i] = mat[i:time_steps+i]\n",
    "            y[i] = mat[time_steps+i+pred_step:time_steps+i+pred_step+pred_len, pred_col_id].reshape(pred_len,)\n",
    "        \n",
    "        x = self.trimDataset(x, batchSize)\n",
    "        y = y[-x.shape[0]:] \n",
    "        np.save(os.path.join(self._snapshotDir,\"xTimeseries\"), x)\n",
    "        np.save(os.path.join(self._snapshotDir,\"yTimeseries\"), y)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    # trims dataset to a size that's divisible by BATCH_SIZE\n",
    "    def trimDataset(self,mat, batch_size):\n",
    "        no_of_rows_drop = mat.shape[0]%batch_size\n",
    "        if(no_of_rows_drop > 0):\n",
    "            return mat[no_of_rows_drop:]\n",
    "        else:\n",
    "            return mat\n",
    "\n",
    "    def buildLSTMModel(self, train_data, pred_len, batch_size, timeSteps):  \n",
    "        model = Sequential()\n",
    "        model.add(LSTM(150, batch_input_shape=(batch_size, timeSteps, \n",
    "        train_data.shape[2]), dropout=0.0, recurrent_dropout=0.0, \n",
    "        stateful=True, kernel_initializer='random_uniform'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(40,activation='relu'))\n",
    "        model.add(Dense(pred_len,activation='sigmoid'))\n",
    "        optimizer = optimizers.RMSprop(lr=0.001)\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "        return model\n",
    "    \n",
    "    def compute_sq_error(self, lstm_model, x_test_data, y_test_data, batchSize, predColId, outfile):\n",
    "        # compute model error\n",
    "        print('Computing model error...')\n",
    "        y_pred = lstm_model.predict(x_test_data, batch_size=batchSize)\n",
    "        y_pred_mean = y_pred.mean(axis=1)\n",
    "        y_test_mean = y_test_data.mean(axis=1)\n",
    "\n",
    "        # de-normalize the prediction\n",
    "        y_pred_prices = (y_pred_mean * self._min_max_scaler.data_range_[predColId]) + self._min_max_scaler.data_min_[predColId] \n",
    "        y_orig_prices = (y_test_mean * self._min_max_scaler.data_range_[predColId]) + self._min_max_scaler.data_min_[predColId]\n",
    "\n",
    "        error = mean_squared_error(y_orig_prices, y_pred_prices)\n",
    "\n",
    "        self.plotRealVsPred(y_pred_prices, y_orig_prices, error, outfile)\n",
    "        return error\n",
    "\n",
    "    def plotAccuracyLoss(self, history, outdir):\n",
    "        # plot loss\n",
    "        plt.figure(figsize=(20,15))\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Model training loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "        outfile = os.path.join(outdir,'training_loss.png')\n",
    "        plt.savefig(outfile, dpi=100)\n",
    "\n",
    "    def plotRealVsPred(self, pred, real, error, outfile):\n",
    "        plt.figure(figsize=(20,15))\n",
    "        plt.plot(pred)\n",
    "        plt.plot(real)\n",
    "        plt.title('Prediction vs Real Stock Price (Sq. Error: ' + str(error) + ')')\n",
    "        plt.ylabel('Price (in SEK)')\n",
    "        plt.xlabel('Minutes')\n",
    "        plt.legend(['Prediction', 'Real'], loc='upper left')\n",
    "        plt.savefig(outfile, dpi = 100)   # save the figure to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCommandLineArgs():\n",
    "    dirPath = os.getcwd()\n",
    "    now=datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    snapshotDir = os.path.join(dirPath, 'results', now)\n",
    "    parser = argparse.ArgumentParser(description='LSTM model to predict next nth min stock price')\n",
    "    parser.add_argument(\n",
    "        '-m',\n",
    "        '--markettype',\n",
    "        default='se',\n",
    "        help='Stock market (string), default=\\\"se\\\"'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-s',\n",
    "        '--stockname',\n",
    "        default='XSPRAY.ST',\n",
    "        help='Stock name (string), default=\\\"XSPRAY.ST\\\"'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-n',\n",
    "        '--predlen',\n",
    "        default='1',\n",
    "        help='Number of steps to predict (int), default=1'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-p',\n",
    "        '--predstep',\n",
    "        default='1',\n",
    "        help='How many steps ahead to predict (int), default=1, i.e., the next minute'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-b',\n",
    "        '--batchsize',\n",
    "        default = '100',\n",
    "        help = 'Batch size for the model (int), default=100'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-d',\n",
    "        '--daysfortesting',\n",
    "        default = '3',\n",
    "        help = 'Number of days to use for testing the model (int), default=3'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-t',\n",
    "        '--timesteps',\n",
    "        default='60',\n",
    "        help='Time steps for the model (int), default=60'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-e',\n",
    "        '--epochs',\n",
    "        default='100',\n",
    "        help='Number of epochs (int), default=100'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-o',\n",
    "        '--outdir',\n",
    "        default=snapshotDir,\n",
    "        help='Output directory for writing model and results (string), default=' + os.path.join(dirPath, 'results', 'current_timestamp')\n",
    "    )\n",
    "    \n",
    "    cmd_args = parser.parse_args()\n",
    "    market_type = str(cmd_args.markettype)\n",
    "    stock_name = str(cmd_args.stockname)\n",
    "    pred_len = int(cmd_args.predlen)\n",
    "    test_days = int(cmd_args.daysfortesting)\n",
    "    pred_step = int(cmd_args.predstep)\n",
    "    batch_size = int(cmd_args.batchsize)\n",
    "    timesteps = int(cmd_args.timesteps)    \n",
    "    epochs = int(cmd_args.epochs)\n",
    "    outdir = str(cmd_args.outdir)\n",
    "\n",
    "    return cmd_args, market_type, stock_name, test_days, pred_len, pred_step, batch_size, timesteps, epochs, outdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import local library functions\n",
    "dirPath = os.getcwd()\n",
    "trainYaml = os.path.join(dirPath,'../config/train.yaml')\n",
    "dbYaml = os.path.join(dirPath, '../config/db.yaml')\n",
    "\n",
    "end_date = datetime.now()\n",
    "\n",
    "# main directory is two steps up\n",
    "basePath = os.path.dirname(os.path.dirname(dirPath))\n",
    "#now=datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "print('Directory path is ' + dirPath)\n",
    "\n",
    "# read command line arguments\n",
    "#cmdArgs, stockMarket, stockName, numOfTestingDays, predictionLength, predictionStep, batchSize, timeSteps, epochs, snapshotDir = readCommandLineArgs()\n",
    "stockMarket = 'se'\n",
    "stockName = 'XSPRAY.ST'\n",
    "numOfTestingDays = 3\n",
    "predictionLength = 1\n",
    "predictionStep = 1\n",
    "batchSize = 100\n",
    "timeSteps = 60\n",
    "epochs = 10\n",
    "snapshotDir = os.path.join(dirPath, '../results/AZN.ST.TS90.BS200.LookAhead1.2021-03-20_11-22-04/')\n",
    "print(snapshotDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(snapshotDir, exist_ok=True)\n",
    "logFile = os.path.join(snapshotDir,'model_output.log')\n",
    "\n",
    "print('################################')\n",
    "print('No. of epochs are ' + str(epochs))\n",
    "\n",
    "# read independent stocks for selected stock\n",
    "with open(trainYaml) as file:\n",
    "    documents = yaml.full_load(file)\n",
    "indStocks = documents[stockName]\n",
    "allStocks = [stockName] + indStocks\n",
    "\n",
    "# read database info\n",
    "with open(dbYaml) as file:\n",
    "    documents = yaml.full_load(file)\n",
    "dbInfo = documents['dbInfo'][stockMarket]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = ModelTrainer(dbHost = os.environ['dbHost'], dbUser = os.environ['dbUser'], \n",
    "dbPasswd = os.environ['dbPasswd'], dbName = dbInfo['db'], dbTable = dbInfo['trainTable'], \n",
    "stocks = allStocks, snapshotDir = snapshotDir)\n",
    "\n",
    "stockDataMelted = mt.readTrainingData(end_date)\n",
    "stockData = stockDataMelted.pivot_table(index='Datetime', columns = 'Ticker')\n",
    "stockData = stockData.fillna(method='ffill')\n",
    "predColId = np.where(pd.MultiIndex.get_level_values(stockData.columns, level = 1) == stockName)[0][0]\n",
    "np.save(os.path.join(snapshotDir, \"stockData\"), stockData.values)\n",
    "\n",
    "# get data for training and validation\n",
    "trainingDays = end_date - timedelta(days=numOfTestingDays)\n",
    "trainData = stockData[stockData.index < trainingDays]\n",
    "validData = stockData.loc[stockData.index.difference(trainData.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to change the absolute stock value to relative increase/decrease with respect to closing price of last day\n",
    "def transform_to_perc(df):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqDays = np.sort(np.unique(trainData.index.date))\n",
    "lastPrice = trainData[trainData.index.date == uniqDays[0]]\n",
    "print(lastPrice)\n",
    "for day in uniqDays[:4]:\n",
    "    print(day - timedelta(days=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xValidation, minMaxScaler = mt.normalizeDataAndSave(trainData, validData)\n",
    "\n",
    "### Training data: build time-series\n",
    "xTimeseries, yTimeseries = mt.buildTimeseries(xTrain, predictionLength, predictionStep, predColId, timeSteps, batchSize)\n",
    "\n",
    "### Validation data: build time-series\n",
    "x_v, y_v = mt.buildTimeseries(xValidation, predictionLength, predictionStep, predColId, timeSteps, batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and run the LSTM model\n",
    "csv_logger = CSVLogger(os.path.join(logFile), append=True)\n",
    "print('Defining model...')\n",
    "lstm_model = mt.buildLSTMModel(xTimeseries, predictionLength, batchSize, timeSteps)\n",
    "\n",
    "print('Training model...')\n",
    "history = lstm_model.fit(xTimeseries, yTimeseries, epochs=epochs, verbose=2, batch_size=batchSize,\n",
    "                    shuffle=False, validation_data=(x_v,y_v), callbacks=[csv_logger])\n",
    "\n",
    "#lstm_model = load_model('/tmp/lstm_model.tf', compile=True)\n",
    "\n",
    "perfPlot = os.path.join(snapshotDir,'perf_plot.jpg')\n",
    "error = mt.compute_sq_error(lstm_model, x_v, y_v, batchSize, predColId, perfPlot)\n",
    "print(\"mean squared error of the model is\", error)\n",
    "\n",
    "# write model to file \n",
    "modelPath = os.path.join(snapshotDir,'trained_model.h5')\n",
    "print('Writing model to ' + modelPath)\n",
    "lstm_model.save(modelPath)\n",
    "\n",
    "print('Analysis done. Check ' + perfPlot + ' for the plot\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}