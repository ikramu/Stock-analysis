{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "from keras import optimizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "sns.set(rc={'figure.figsize':(15,8)})\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-26 09:00:00+01:00</td>\n",
       "      <td>130.399994</td>\n",
       "      <td>130.860001</td>\n",
       "      <td>129.979996</td>\n",
       "      <td>130.860001</td>\n",
       "      <td>130.860001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-26 09:01:00+01:00</td>\n",
       "      <td>130.820007</td>\n",
       "      <td>131.779999</td>\n",
       "      <td>129.860001</td>\n",
       "      <td>129.860001</td>\n",
       "      <td>129.860001</td>\n",
       "      <td>37317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-26 09:02:00+01:00</td>\n",
       "      <td>129.820007</td>\n",
       "      <td>130.179993</td>\n",
       "      <td>128.580002</td>\n",
       "      <td>129.580002</td>\n",
       "      <td>129.580002</td>\n",
       "      <td>18178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-26 09:03:00+01:00</td>\n",
       "      <td>129.619995</td>\n",
       "      <td>130.419998</td>\n",
       "      <td>129.559998</td>\n",
       "      <td>130.039993</td>\n",
       "      <td>130.039993</td>\n",
       "      <td>13635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-26 09:04:00+01:00</td>\n",
       "      <td>130.419998</td>\n",
       "      <td>130.419998</td>\n",
       "      <td>129.179993</td>\n",
       "      <td>129.179993</td>\n",
       "      <td>129.179993</td>\n",
       "      <td>37434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Datetime        Open        High         Low       Close  \\\n",
       "0  2020-03-26 09:00:00+01:00  130.399994  130.860001  129.979996  130.860001   \n",
       "1  2020-03-26 09:01:00+01:00  130.820007  131.779999  129.860001  129.860001   \n",
       "2  2020-03-26 09:02:00+01:00  129.820007  130.179993  128.580002  129.580002   \n",
       "3  2020-03-26 09:03:00+01:00  129.619995  130.419998  129.559998  130.039993   \n",
       "4  2020-03-26 09:04:00+01:00  130.419998  130.419998  129.179993  129.179993   \n",
       "\n",
       "    Adj Close  Volume  \n",
       "0  130.860001       0  \n",
       "1  129.860001   37317  \n",
       "2  129.580002   18178  \n",
       "3  130.039993   13635  \n",
       "4  129.179993   37434  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hm = pd.read_csv('data/HM-B.ST_1m.csv')\n",
    "#hm = hm.set_index('Datetime')\n",
    "hm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_daily_stock_movement(df, stock_name, var, plt):\n",
    "    curdate = df['Date'].unique()\n",
    "    df = df.set_index(df['Time'])\n",
    "    md = str(df['Date'][0])\n",
    "    plot_title = stock_name + ' stock for ' + md\n",
    "    if(len(curdate) > 1):\n",
    "        print('Data consists of more than one day. Exiting...')\n",
    "    else:\n",
    "        #plt.figure()\n",
    "        #plt.plot(df[\"Open\"])\n",
    "        plt.plot(df[var])\n",
    "        #plt.plot(df[\"Volume\"])\n",
    "        #plt.plot(df[\"Close\"])\n",
    "        plt.set_title(plot_title)\n",
    "        plt.set_ylabel('Price (SEK)')\n",
    "        #plt.set_ylim([120,190])\n",
    "        plt.set_xlabel('Minutes')\n",
    "        plt.legend(['Open','High','Low','Close'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm2 = hm.copy()\n",
    "hm2[\"Datetime\"] = pd.to_datetime(hm2[\"Datetime\"])\n",
    "hm2['Date'] = hm2['Datetime'].dt.date\n",
    "hm2['Time'] = hm2['Datetime'].dt.time\n",
    "day_list = hm2['Date'].unique()\n",
    "\n",
    "#nrows = int(np.ceil(len(day_list)/2))\n",
    "nrows = len(day_list)\n",
    "\n",
    "fig, axs = plt.subplots(nrows,2, figsize=(20, 60), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace =0.5, wspace=0.3)\n",
    "\n",
    "axs = axs.ravel()\n",
    "for i in range(len(day_list)):\n",
    "    curdf = hm2[hm2.Date == day_list[i]]\n",
    "    plot_daily_stock_movement(curdf, 'H&M', 'Close', axs[2*i])\n",
    "    plot_daily_stock_movement(curdf, 'H&M', 'Volume', axs[2*i+1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily stock movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(hm[\"Open\"])\n",
    "plt.plot(hm[\"High\"])\n",
    "plt.plot(hm[\"Low\"])\n",
    "plt.plot(hm[\"Close\"])\n",
    "plt.title('H&M stock price history')\n",
    "plt.ylabel('Price (SEK)')\n",
    "plt.xlabel('Minutes')\n",
    "plt.legend(['Open','High','Low','Close'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(hm[\"Volume\"])\n",
    "plt.title('H&M stock volume history')\n",
    "plt.ylabel('Volume')\n",
    "plt.xlabel('Datetime')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(minutedata):\n",
    "    train_cols = [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]\n",
    "    df_train, df_test = train_test_split(minutedata, train_size=0.9, test_size=0.1, shuffle=False)\n",
    "    print(\"Train and Test size are: \", len(df_train), \" and \", len(df_test))\n",
    "    \n",
    "    # scale the feature MinMax, build array\n",
    "    x = df_train.loc[:,train_cols].values\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    x_train = min_max_scaler.fit_transform(x)\n",
    "    x_test = min_max_scaler.transform(df_test.loc[:,train_cols])\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest = normalize_data(hm)\n",
    "xtrain.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]\n",
    "df_train, df_test = train_test_split(hm, train_size=0.9, test_size=0.1, shuffle=False)\n",
    "print(\"Train and Test size are: \", len(df_train), \" and \", len(df_test))\n",
    "# scale the feature MinMax, build array\n",
    "x = df_train.loc[:,train_cols].values\n",
    "min_max_scaler = MinMaxScaler()\n",
    "x_train = min_max_scaler.fit_transform(x)\n",
    "x_test = min_max_scaler.transform(df_test.loc[:,train_cols])\n",
    "\n",
    "# we will look at last hour to predict next value, so time step = 60\n",
    "TIME_STEPS = 30\n",
    "BATCH_SIZE = 100\n",
    "OUTPUT_PATH='log/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_timeseries(mat, y_col_index):\n",
    "    # y_col_index is the index of column that would act as output column\n",
    "    # total number of time-series samples would be len(mat) - TIME_STEPS\n",
    "    dim_0 = mat.shape[0] - TIME_STEPS\n",
    "    dim_1 = mat.shape[1]\n",
    "    x = np.zeros((dim_0, TIME_STEPS, dim_1))\n",
    "    y = np.zeros((dim_0,))\n",
    "    \n",
    "    for i in tqdm_notebook(range(dim_0)):\n",
    "        x[i] = mat[i:TIME_STEPS+i]\n",
    "        y[i] = mat[TIME_STEPS+i, y_col_index]\n",
    "    print(\"length of time-series i/o\",x.shape,y.shape)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_dataset(mat, batch_size):\n",
    "    \"\"\"\n",
    "    trims dataset to a size that's divisible by BATCH_SIZE\n",
    "    \"\"\"\n",
    "    no_of_rows_drop = mat.shape[0]%batch_size\n",
    "    if(no_of_rows_drop > 0):\n",
    "        return mat[:-no_of_rows_drop]\n",
    "    else:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_lstm(idata, label_col_id, BATCH_SIZE):\n",
    "    x_ts, y_ts = build_timeseries(x_train, label_col_id)\n",
    "    x_ts = trim_dataset(x_ts, BATCH_SIZE)\n",
    "    y_ts = trim_dataset(y_ts, BATCH_SIZE)\n",
    "    return x_ts, y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t, y_t = prepare_data_for_lstm(x_train, 3, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t, y_t = build_timeseries(x_train, 3)\n",
    "x_t = trim_dataset(x_t, BATCH_SIZE)\n",
    "y_t = trim_dataset(y_t, BATCH_SIZE)\n",
    "x_temp, y_temp = build_timeseries(x_test, 3)\n",
    "x_val, x_test_t = np.split(trim_dataset(x_temp, BATCH_SIZE),2)\n",
    "y_val, y_test_t = np.split(trim_dataset(y_temp, BATCH_SIZE),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "from keras import optimizers\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(100, batch_input_shape=(BATCH_SIZE, TIME_STEPS, x_t.shape[2]), dropout=0.0, recurrent_dropout=0.0, stateful=True,     kernel_initializer='random_uniform'))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(20,activation='relu'))\n",
    "lstm_model.add(Dense(1,activation='sigmoid'))\n",
    "optimizer = optimizers.RMSprop(lr=0.001)\n",
    "lstm_model.compile(loss='mean_squared_error', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger(os.path.join(OUTPUT_PATH, 'lstm_hm' + '.log'), append=True)\n",
    "\n",
    "history = lstm_model.fit(x_t, y_t, epochs=100, verbose=2, batch_size=BATCH_SIZE,\n",
    "                    shuffle=False, validation_data=(trim_dataset(x_val, BATCH_SIZE),\n",
    "                    trim_dataset(y_val, BATCH_SIZE)), callbacks=[csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lstm_model.predict(trim_dataset(x_test_t, BATCH_SIZE), batch_size=BATCH_SIZE)\n",
    "y_pred = y_pred.flatten()\n",
    "y_test_t = trim_dataset(y_test_t, BATCH_SIZE)\n",
    "error = mean_squared_error(y_test_t, y_pred)\n",
    "print(\"Error is\", error, y_pred.shape, y_test_t.shape)\n",
    "print(y_pred[0:15])\n",
    "print(y_test_t[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_org = (y_pred * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] \n",
    "y_test_t_org = (y_test_t * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] # min_max_scaler.inverse_transform(y_test_t)\n",
    "print(y_pred_org[0:15])\n",
    "print(y_test_t_org[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(y_pred_org)\n",
    "plt.plot(y_test_t_org)\n",
    "plt.title('Prediction vs Real Stock Price')\n",
    "plt.ylabel('Price (in SEK)')\n",
    "plt.xlabel('Minutes')\n",
    "plt.legend(['Prediction', 'Real'], loc='upper left')\n",
    "plt.show()\n",
    "#plt.savefig(os.path.join(OUTPUT_PATH, 'pred_vs_real_BS'+str(BATCH_SIZE)+\"_\"+time.ctime()+'.png'))\n",
    "#print_time(\"program completed \", stime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM for outputting next five values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_timeseries(mat, y_col_index):\n",
    "    # y_col_index is the index of column that would act as output column\n",
    "    # total number of time-series samples would be len(mat) - TIME_STEPS\n",
    "    dim_0 = mat.shape[0] - TIME_STEPS\n",
    "    dim_1 = mat.shape[1]\n",
    "    x = np.zeros((dim_0, TIME_STEPS, dim_1))\n",
    "    y = np.zeros((dim_0,5))\n",
    "    print(y.shape)\n",
    "    print(mat[TIME_STEPS+1:TIME_STEPS+1+5, y_col_index].reshape(1,5).shape)\n",
    "    \n",
    "    for i in tqdm_notebook(range(dim_0-5)):\n",
    "        x[i] = mat[i:TIME_STEPS+i]\n",
    "        #print(y[i].shape)\n",
    "        y[i] = mat[TIME_STEPS+i:TIME_STEPS+i+5, y_col_index].reshape(5,)\n",
    "    print(\"length of time-series i/o\",x.shape,y.shape)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_lstm(idata, label_col_id, BATCH_SIZE):\n",
    "    x_ts, y_ts = build_timeseries(x_train, label_col_id)\n",
    "    x_ts = trim_dataset(x_ts, BATCH_SIZE)\n",
    "    y_ts = trim_dataset(y_ts, BATCH_SIZE)\n",
    "    return x_ts, y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = prepare_data_for_lstm(x_train, 3, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t, y_t = build_timeseries(x_train, 3)\n",
    "x_t = trim_dataset(x_t, BATCH_SIZE)\n",
    "y_t = trim_dataset(y_t, BATCH_SIZE)\n",
    "x_temp, y_temp = build_timeseries(x_test, 3)\n",
    "x_val, x_test_t = np.split(trim_dataset(x_temp, BATCH_SIZE),2)\n",
    "y_val, y_test_t = np.split(trim_dataset(y_temp, BATCH_SIZE),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(train_data, BATCH_SIZE, TIME_STEPS):  \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, batch_input_shape=(BATCH_SIZE, TIME_STEPS, train_data.shape[2]), dropout=0.0, recurrent_dropout=0.0, stateful=True, kernel_initializer='random_uniform'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20,activation='relu'))\n",
    "    model.add(Dense(5,activation='sigmoid'))\n",
    "    optimizer = optimizers.RMSprop(lr=0.001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = build_lstm_model(x_t, BATCH_SIZE, TIME_STEPS)\n",
    "print(lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "from keras import optimizers\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(100, batch_input_shape=(BATCH_SIZE, TIME_STEPS, x_t.shape[2]), dropout=0.0, recurrent_dropout=0.0, stateful=True, kernel_initializer='random_uniform'))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(20,activation='relu'))\n",
    "lstm_model.add(Dense(5,activation='sigmoid'))\n",
    "optimizer = optimizers.RMSprop(lr=0.001)\n",
    "lstm_model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger(os.path.join(OUTPUT_PATH, 'lstm_hm' + '.log'), append=True)\n",
    "\n",
    "history = lstm_model.fit(x_t, y_t, epochs=100, verbose=2, batch_size=BATCH_SIZE,\n",
    "                    shuffle=False, validation_data=(trim_dataset(x_val, BATCH_SIZE),\n",
    "                    trim_dataset(y_val, BATCH_SIZE)), callbacks=[csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sq_error(lstm_model, x_test_data, y_test_data, BATCH_SIZE):\n",
    "    y_pred = lstm_model.predict(trim_dataset(x_test_data, BATCH_SIZE), batch_size=BATCH_SIZE)\n",
    "    y_pred_mean = y_pred.mean(axis=1)\n",
    "    y_test_t = trim_dataset(y_test_data, BATCH_SIZE)\n",
    "    y_test_t_mean = y_test_t.mean(axis=1)\n",
    "    error = mean_squared_error(y_test_t_mean, y_pred_mean)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_error = compute_sq_error(lstm_model, x_test_t, y_test_t, BATCH_SIZE)\n",
    "print(\"Error is\", sq_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lstm_model.predict(trim_dataset(x_test_t, BATCH_SIZE), batch_size=BATCH_SIZE)\n",
    "y_pred_mean = y_pred.mean(axis=1)\n",
    "#y_pred = y_pred.flatten()\n",
    "y_test_t = trim_dataset(y_test_t, BATCH_SIZE)\n",
    "y_test_t_mean = y_test_t.mean(axis=1)\n",
    "error = mean_squared_error(y_test_t_mean, y_pred_mean)\n",
    "print(\"Error is\", error, y_pred_mean.shape, y_test_t_mean.shape)\n",
    "#print(y_pred[0:15])\n",
    "#print(y_test_t[0:15])\n",
    "float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_org = (y_pred_mean * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] \n",
    "y_test_t_org = (y_test_t_mean * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3] # min_max_scaler.inverse_transform(y_test_t)\n",
    "print(y_pred_org[0:15])\n",
    "print(y_test_t_org[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_real_vs_pred(pred, real):\n",
    "    outfile = 'figures/TS_'+str(TIME_STEPS)+'_BS_'+str(BATCH_SIZE)+'.png'\n",
    "    plt.figure()\n",
    "    plt.plot(pred)\n",
    "    plt.plot(real)\n",
    "    plt.title('Prediction vs Real Stock Price')\n",
    "    plt.ylabel('Price (in SEK)')\n",
    "    plt.xlabel('Minutes')\n",
    "    plt.legend(['Prediction', 'Real'], loc='upper left')\n",
    "    fig.savefig(outfile)   # save the figure to file\n",
    "    plt.close(fig)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_real_vs_pred(y_pred_org, y_test_t_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots( nrows=1, ncols=1 )\n",
    "plt.figure()\n",
    "plt.plot(y_pred_org)\n",
    "plt.plot(y_test_t_org)\n",
    "plt.title('Prediction vs Real Stock Price')\n",
    "plt.ylabel('Price (in SEK)')\n",
    "plt.xlabel('Minutes')\n",
    "plt.legend(['Prediction', 'Real'], loc='upper left')\n",
    "plt.savefig('/tmp/ab.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('figures/TS_'+str(TIME_STEPS)+'_BS_'+str(BATCH_SIZE)+'_Error_'+str(round(error,5))+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm2 = hm.copy()\n",
    "hm2[\"Datetime\"] = pd.to_datetime(hm2[\"Datetime\"])\n",
    "hm2['Date'] = hm2['Datetime'].dt.date\n",
    "hm2['Time'] = hm2['Datetime'].dt.time\n",
    "print(len(hm2['Date'].unique()))\n",
    "#hm2.sort_values(by=['Datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm3 = hm\n",
    "#hm3['Date'] = hm3['Datetime'].date()\n",
    "hm3 = hm3.set_index('Datetime')\n",
    "shm = hm3['2020-03-02']\n",
    "md = shm['Date'].unique()\n",
    "#dt.strftime('%m/%d/%Y')\n",
    "print(shm['Date'][0].strftime('%m/%d/%Y'))\n",
    "#hm2.head()\n",
    "plot_daily_stock_movement(hm3['2020-03-02'], 'H&M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(pd.Series(np.random.normal(10,3,size=10)), color='g')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(pd.Series(np.random.normal(0,1,size=10)), color='r')\n",
    "#ax2.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(np.random.normal(50, 30,[350,1]), \n",
    "        index=pd.date_range('1/1/2000', periods=350), columns=list('A'))\n",
    "df2 = pd.DataFrame(np.random.normal(550, 50,[350,1]), \n",
    "        index=pd.date_range('1/1/2000', periods=350), columns=list('D'))\n",
    "#df3 = pd.concat([df, df2], sort = True)\n",
    "df3 = pd.concat([df.reset_index(drop=True), df2.reset_index(drop=True)], axis=1)\n",
    "print(df3.head())\n",
    "#df = df.cumsum()\n",
    "df3.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.normal(50, 2, [3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sel_data(stock_name, interval = '1m', datadir = 'data/'):\n",
    "    colnames = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
    "    filename = datadir + stock_name + '_' + interval + '.csv'\n",
    "    data = pd.read_csv(filename)\n",
    "    cols = list(data.columns)[1:] \n",
    "    if colnames == cols:\n",
    "        ll = list([stock_name+'_'+x for x in cols])\n",
    "        cnames = ['Datetime'] + ll\n",
    "        data.columns = cnames\n",
    "    else:\n",
    "        print('Columns are not in order')\n",
    "    # select only the Adj Close and Volume\n",
    "    if re.search('\\^', stock_name) is None:\n",
    "        data = data.iloc[:, [0,5,6]]\n",
    "    else:\n",
    "        data = data.iloc[:, [0,5]]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm = read_sel_data('HM-B.ST')\n",
    "hm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_list = ['ABB.ST', 'ASSA-B.ST', 'ATCO-B.ST', 'ELUX-B.ST', 'ERIC-B.ST', 'HM-B.ST', 'SAND.ST', '^OMXC20', '^OMXC25', '^OMXH25', '^OMXHPI', '^OMX']\n",
    "sel_list = ['HM-B.ST','^OMXC20', '^OMXC25', '^OMXH25', '^OMXHPI', '^OMX']\n",
    "interval = '1m'\n",
    "hh = dict()\n",
    "hh['a'] = [1,2,3]\n",
    "print(hh['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "rr=re.search('\\^', sel_list[0])\n",
    "rr is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dic = dict()\n",
    "for i in sel_list:\n",
    "    print('reading data for ' + i)\n",
    "    data_dic[i] = read_sel_data(i)\n",
    "    \n",
    "print('done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dic['HM-B.ST'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = data_dic[sel_list[0]]\n",
    "\n",
    "for i in range(1, len(sel_list)):\n",
    "    merged_df = merged_df.merge(data_dic[sel_list[i]], on = 'Datetime')\n",
    "merged_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
    "ll = list(['HM_'+x for x in colnames])\n",
    "cnames = ['Datatime'] + ll\n",
    "hm2 = hm.copy()\n",
    "hm2.columns = cnames\n",
    "hm2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sel_data(stock_name, interval = '1m', datadir = 'data/'):\n",
    "    colnames = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
    "    filename = datadir + stock_name + '_' + interval + '.csv'\n",
    "    data = pd.read_csv(filename)\n",
    "    cols = list(data.columns)[1:] \n",
    "    if colnames == cols:\n",
    "        ll = list([stock_name+'_'+x for x in cols])\n",
    "        cnames = ['Datetime'] + ll\n",
    "        data.columns = cnames\n",
    "    else:\n",
    "        print('Columns are not in order')\n",
    "    # select only the Adj Close and Volume\n",
    "    if re.search('\\^', stock_name) is None:\n",
    "        data = data.iloc[:, [0,5,6]]\n",
    "    else:\n",
    "        data = data.iloc[:, [0,5]]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_full_data(stock_list):\n",
    "    data_dic = dict()\n",
    "    for i in stock_list:\n",
    "        print('reading data for ' + i)\n",
    "        data_dic[i] = read_sel_data(i)\n",
    "    \n",
    "    merged_df = data_dic[stock_list[0]]\n",
    "\n",
    "    for i in range(1, len(stock_list)):\n",
    "        merged_df = merged_df.merge(data_dic[stock_list[i]], on = 'Datetime')\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = 'ERIC-B.ST'\n",
    "# let's add exchange indices \n",
    "ind_list = list(['^OMXC20', '^OMXC25', '^OMXH25', '^OMXHPI', '^OMX'])\n",
    "comp_list = list([stock_name]) + ind_list\n",
    "dd = get_full_data(comp_list)\n",
    "dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.columns[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
